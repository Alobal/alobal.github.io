<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="hphqb-Td3Fl9WUNX1Pj-X3Y9yfsqpQUnH4eP6eAqS7Q">
  <meta name="baidu-site-verification" content="codeva-9sj0MAnGdB">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"sitchzou.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":10,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="点云空间学习 PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation  Qi C R, Su H, Mo K, et al. Pointnet: Deep learning on point sets for 3d classification and segmentation[C]&#x2F;&#x2F;Pr">
<meta property="og:type" content="article">
<meta property="og:title" content="点云深度学习论文记录 (持续更新)">
<meta property="og:url" content="https://sitchzou.com/2022/01/15/%E7%82%B9%E4%BA%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95/index.html">
<meta property="og:site_name" content="Sitch&#39;s Blog">
<meta property="og:description" content="点云空间学习 PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation  Qi C R, Su H, Mo K, et al. Pointnet: Deep learning on point sets for 3d classification and segmentation[C]&#x2F;&#x2F;Pr">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.imgur.com/byWCQ3e.png">
<meta property="og:image" content="https://i.imgur.com/CKOuOuc.png">
<meta property="og:image" content="https://i.imgur.com/iYWt5q9.png">
<meta property="og:image" content="https://i.imgur.com/AsMxFcH.png">
<meta property="og:image" content="https://i.imgur.com/Qj5RKxE.png">
<meta property="og:image" content="https://i.imgur.com/hRhgDLA.png">
<meta property="og:image" content="https://i.imgur.com/CSh1yZu.png">
<meta property="og:image" content="https://i.imgur.com/fcsguio.png">
<meta property="og:image" content="https://i.imgur.com/aGKzR83.png">
<meta property="og:image" content="https://i.imgur.com/Kr9tDFu.png">
<meta property="og:image" content="https://i.imgur.com/502jS7L.png">
<meta property="og:image" content="https://i.imgur.com/RH31S9k.png">
<meta property="og:image" content="https://i.imgur.com/b4YiNR1.png">
<meta property="og:image" content="https://i.imgur.com/mLky44z.png">
<meta property="og:image" content="https://i.imgur.com/IfVisYX.png">
<meta property="og:image" content="https://i.imgur.com/dLwfRQ3.png">
<meta property="og:image" content="https://i.imgur.com/sCYRIUn.png">
<meta property="og:image" content="https://i.imgur.com/s3Mi1Q7.png">
<meta property="og:image" content="https://i.imgur.com/gTXIINx.png">
<meta property="og:image" content="https://i.imgur.com/UJetjfg.png">
<meta property="og:image" content="https://i.imgur.com/20tjGyc.png">
<meta property="og:image" content="https://i.imgur.com/6A31Qpn.png">
<meta property="og:image" content="https://i.imgur.com/NE0XABU.png">
<meta property="og:image" content="https://i.imgur.com/zXpHetx.png">
<meta property="og:image" content="https://i.imgur.com/gtCaHFs.png">
<meta property="og:image" content="https://i.imgur.com/ohfXXMp.png">
<meta property="og:image" content="https://i.imgur.com/1OGOb3t.png">
<meta property="og:image" content="https://i.imgur.com/K8I1K4I.png">
<meta property="og:image" content="https://i.imgur.com/RhcgMtJ.png">
<meta property="og:image" content="https://i.imgur.com/i9xSHaC.png">
<meta property="og:image" content="https://i.imgur.com/by9hU75.png">
<meta property="og:image" content="https://i.imgur.com/CbvUBIo.png">
<meta property="og:image" content="https://i.imgur.com/msRVrEW.png">
<meta property="og:image" content="https://i.imgur.com/2tkPCO8.png">
<meta property="og:image" content="https://i.imgur.com/Fi3BjWH.png">
<meta property="og:image" content="https://i.imgur.com/eqmq6lr.png">
<meta property="og:image" content="https://i.imgur.com/QWhZZWr.png">
<meta property="og:image" content="https://i.imgur.com/0Cjr6hr.png">
<meta property="article:published_time" content="2022-01-15T03:24:12.000Z">
<meta property="article:modified_time" content="2022-01-15T03:24:12.000Z">
<meta property="article:author" content="Sitch">
<meta property="article:tag" content="序列学习">
<meta property="article:tag" content="点云">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.imgur.com/byWCQ3e.png">

<link rel="canonical" href="https://sitchzou.com/2022/01/15/%E7%82%B9%E4%BA%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>点云深度学习论文记录 (持续更新) | Sitch's Blog</title>
  


  <script data-pjax>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?13a5881f99caf50927823ae25a7cb3ee";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Sitch's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-movies">

    <a href="/movies/" rel="section"><i class="fa fa-film fa-fw"></i>影片</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sitchzou.com/2022/01/15/%E7%82%B9%E4%BA%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Sitch">
      <meta itemprop="description" content="做好自己的现在">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sitch's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          点云深度学习论文记录 (持续更新)
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-15 11:24:12" itemprop="dateCreated datePublished" datetime="2022-01-15T11:24:12+08:00">2022-01-15</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/01/15/%E7%82%B9%E4%BA%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/01/15/%E7%82%B9%E4%BA%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="点云空间学习">点云空间学习</h2>
<h3
id="pointnet-deep-learning-on-point-sets-for-3d-classification-and-segmentation">PointNet:
Deep Learning on Point Sets for 3D Classification and Segmentation</h3>
<ul>
<li>Qi C R, Su H, Mo K, et al. Pointnet: Deep learning on point sets for
3d classification and segmentation[C]//Proceedings of the IEEE
conference on computer vision and pattern recognition. 2017:
652-660.</li>
</ul>
<p>PointNet首次基于原始点云进行深度学习,其提出了点云深度学习的<strong>三大原则:
无序性、点间联系、变换一致性</strong>。基于此,
PointNet在点云上逐点运用了MLP进行变换,
并且构造了<strong>T-Net</strong>进行对抗点云的仿射变换, 最终使用max
pool进行对称聚合。</p>
<blockquote>
<p>缺少对局部结构的特征学习</p>
</blockquote>
<h3
id="pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space">PointNet++:
Deep Hierarchical Feature Learning on Point Sets in a Metric Space</h3>
<ul>
<li>Qi C R, Yi L, Su H, et al. Pointnet++: Deep hierarchical feature
learning on point sets in a metric space[J]. Advances in neural
information processing systems, 2017, 30.</li>
</ul>
<p>PointNet没有捕捉到点的局部结构特征，限制了细粒度和复杂场景的识别、泛化能力。PointNet++则引出了一个<strong>set
abstraction层</strong>对点云进行多级学习。set
abstraction定义了多级多块的局部邻域结构,
其在每一个局部邻域中都使用了mini-PointNet来进行特征抽取。然而由于点云是非均匀分布的,
不同的局部邻域的密度不一样,
因此PointNet++提出了两种自适应密度的特征融合模块: <strong>Multi-scale
grouping（MSG）</strong> 和 <strong>Multi-resolution
grouping（MRG）</strong>。</p>
<p>另外由于部位分割等任务最终需要输出逐点的特征标签, 因此在set
abstraction之后, Pointnet++一方面在同一级内进行反距离的插值传播,
另一方面自顶向下进行反向逐级的特征传播。在同一层内对两种传播特征进行拼接,
即得到该层的逐点特征。</p>
<span id="more"></span>
<h3 id="pointconv">PointConv</h3>
<ul>
<li>Wu W, Qi Z, Fuxin L. Pointconv: Deep convolutional networks on 3d
point clouds[C]//Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 2019: 9621-9630.</li>
</ul>
<h3 id="pointcnn">PointCNN</h3>
<ul>
<li>Li Y, Bu R, Sun M, et al. Pointcnn: Convolution on x-transformed
points[J]. Advances in neural information processing systems, 2018,
31.</li>
</ul>
<h3 id="kpconv">KPConv</h3>
<ul>
<li>Thomas H, Qi C R, Deschaud J E, et al. Kpconv: Flexible and
deformable convolution for point clouds[C]//Proceedings of the IEEE/CVF
international conference on computer vision. 2019: 6411-6420.</li>
</ul>
<h3
id="pointnet-based-hand-gesture-recognition">PointNet-Based-Hand-Gesture-Recognition</h3>
<ul>
<li>Mirsu R, Simion G, Caleanu C D, et al. A pointnet-based solution for
3d hand gesture recognition[J]. Sensors, 2020, 20(11): 3226.</li>
<li>SCIE</li>
</ul>
<p>工程论文, 其详细描述了如何对3D点云进行预处理, 提取手势,
最终进入PointNet进行特征提取。</p>
<h3 id="pointweb">PointWeb</h3>
<ul>
<li>Zhao H, Jiang L, Fu C W, et al. Pointweb: Enhancing local
neighborhood features for point cloud processing[C]//Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition. 2019:
5565-5573.</li>
<li>MIT</li>
</ul>
<h3 id="deep-hough-voting-for-3d-object-detection-in-point-clouds">Deep
Hough Voting for 3D Object Detection in Point Clouds</h3>
<ul>
<li>Qi C R, Litany O, He K, et al. Deep hough voting for 3d object
detection in point clouds[C]//proceedings of the IEEE/CVF International
Conference on Computer Vision. 2019: 9277-9286.</li>
</ul>
<h3 id="pct-point-cloud-transformer">PCT: Point cloud transformer</h3>
<ul>
<li>Guo M H, Cai J X, Liu Z N, et al. PCT: Point cloud transformer[J].
Computational Visual Media, 2021, 7(2): 187-199.</li>
<li>清华</li>
</ul>
<p>提出了基于Transformer的PCT网络。Transformer在NLP和图像处理取得了巨大成功，其内在的置换不变性也十分适合点云学习。为了更好的捕捉点云局部信息，使用了最远点采样和最近邻搜索来加强输入的embedding处理。实验证明PCT达到了分类分割和法向估计的SOTA。</p>
<p>由于点云和自然语言是完全不同的数据类型，因此PCT对Transformer作出了几项调整：</p>
<ul>
<li><strong>Coordinate-based input
embedding</strong>：Transformer里的positional encoding
是为了区分不同位置的同一个词。然而点云没有位置顺序关系，因此PCT中将
positional encoding 和 input embedding
结合了起来，基于坐标进行编码。</li>
<li><strong>Optimized offset-attention module</strong>：是原始
self-attention 的升级模块。它把原来的attention
feature换成了self-attention的输入和attention
feature之间的offset。同一个物体在不同的变换下的绝对坐标完全不一样，因此相对坐标更鲁棒。</li>
<li><strong>Neighbor embedding module</strong>：
注意力机制有效捕捉全局特征，但可能忽视局部几何信息，而这在点云中很重要。句子中的每个单独的词都有基本的语义信息，但是点云中孤立的点不存在语义信息。因此使用了一个neighbor
embedding
策略来进行改良，让注意力机制着重于分析点局部邻域的信息，而不是孤立的点的信息。</li>
</ul>
<h3 id="point-transformer">Point Transformer</h3>
<ul>
<li>Zhao H, Jiang L, Jia J, et al. Point transformer[C]//Proceedings of
the IEEE/CVF International Conference on Computer Vision. 2021:
16259-16268.</li>
<li>港中文</li>
</ul>
<p>self-attention是天然的一个集合操作：将位置信息作为元素属性，并且视作集合处理。而另一方面点云天然就是位置属性的集合，因此self-attention直觉上很适合点云数据。之前已经有一些工作在点云分析上使用了attention。他们在整个点云上使用全局的注意力机制，而这会带来昂贵的计算。并且他们使用了标量点积的注意力，即不同通道之间共享相同的聚合权重。</p>
<p>相反，Point Transformer有以下优势：</p>
<ul>
<li><strong>局部应用注意力机制</strong>，使得拥有处理百万点数的大场景的能力。</li>
<li>使用了<strong>vector
attention</strong>，而这是实现高准确率的重要因素。</li>
<li>阐述了<strong>position
encoding</strong>的重要性，而不是像之前的工作一样忽略的位置信息。</li>
</ul>
<h3 id="rs-conv">RS-Conv</h3>
<ul>
<li>CVPR 2019</li>
<li>中国科学院大学，人工智能学院</li>
</ul>
<figure>
<img src="https://i.imgur.com/byWCQ3e.png" alt="Rs-Conv结构" />
<figcaption aria-hidden="true">Rs-Conv结构</figcaption>
</figure>
<p>相比于传统的卷积结构<span class="math inline">\(W_j *
f_j\)</span>，其使用了<span
class="math inline">\(W_{ij}\)</span>来代替<span
class="math inline">\(W_j\)</span>，本质上希望通过一个<span
class="math inline">\(\mathcal{M}\)</span>来学习到预先定义的关系向量<span
class="math inline">\(h_{ij}\)</span>的特征，而这个<span
class="math inline">\(\mathcal{M}\)</span>的实现就是一个point-level的MLP。<span
class="math inline">\(h_{ij}\)</span>比较常用的定义就是3D欧拉距离。</p>
<p><span class="math display">\[
\mathcal{T}\left(\mathbf{f}_{x_j}\right)=\mathbf{w}_{i j} \cdot
\mathbf{f}_{x_j}=\mathcal{M}\left(\mathbf{h}_{i j}\right) \cdot
\mathbf{f}_{x_j}
\]</span></p>
<h3 id="pointcmt">PointCMT</h3>
<ul>
<li>Yan X, Zhan H, Zheng C, et al. Let Images Give You More: Point Cloud
Cross-Modal Training for Shape Analysis[J]. arXiv preprint
arXiv:2210.04208, 2022.</li>
<li>港中文</li>
</ul>
<figure>
<img src="https://i.imgur.com/CKOuOuc.png" alt="PointCMT" />
<figcaption aria-hidden="true">PointCMT</figcaption>
</figure>
<p>单模态的点云模型已经基本走到尽头，如何利用多模态数据(如图片)更好地提升性能呢？一种直接的思路是构建多模态的特征融合网络，但这一方面要特地构建多模态模型结构，另一方面在推理阶段往往难以获得多模态数据。启发于知识蒸馏领域，这篇论文将多模态模型问题转化为知识蒸馏的"老师与学生"问题。PointCMT的知识蒸馏框架可以移植到任意Point模型上快速构建提升。主要步骤：</p>
<ol type="1">
<li>预训练一个大型image-based编码器模型，作为老师。</li>
<li>训练图像到点云的解码器CMPG：即将image-based编码的<span
class="math inline">\(D\)</span>维特征映射成<span
class="math inline">\(N \times 3\)</span>的点云形状</li>
<li>点云模型接收上一步生成的点云作为输入进行训练，并且通过两个增强的损失函数Feature
Enhancement和Classifier Enhancement加强训练。</li>
</ol>
<p>在Feature
Enhancement监督下的CMPG：本质上是一个以EMD为损失函数的映射训练</p>
<p><span class="math display">\[
\mathcal{L}_{\mathrm{EMD}}\left(\mathcal{P}, \hat{\mathcal{P}}^{i m
g}\right)=\min _\phi \sum_{p \in \mathcal{P}}\|p-\phi(p)\|
\]</span></p>
<p>Classifier Enhancement
Loss：本质上就是迫使点云模型的输出靠近图像模型。注意图像特征依然交给点云模型进行使用，并且在训练中不对图像模型进行训练。</p>
<p><span class="math display">\[
\mathcal{L}_{\text {Classifier }}=\mathcal{D}_{K L}\left(\mathrm{Cls}^{p
t s}\left(\mathcal{F}^{i m g}\right) \| \mathrm{Cls}^{p t
s}\left(\mathcal{F}^{p t s}\right)\right)
\]</span></p>
<h3 id="dual-transformer">Dual Transformer</h3>
<ul>
<li>Han X F, Jin Y F, Cheng H X, et al. Dual transformer for point cloud
analysis[J]. IEEE Transactions on Multimedia, 2022.</li>
<li>1区</li>
<li>西南大学CS</li>
</ul>
<p>本质上是将feature level的注意力和channel
level注意力融合在了一起。</p>
<figure>
<img src="https://i.imgur.com/iYWt5q9.png" alt="Dual Transformer" />
<figcaption aria-hidden="true">Dual Transformer</figcaption>
</figure>
<h3 id="pointswd">PointSWD</h3>
<ul>
<li><p>Nguyen T, Pham Q H, Le T, et al. Point-set distances for learning
representations of 3d point clouds[C]//Proceedings of the IEEE/CVF
International Conference on Computer Vision. 2021: 10478-10487.</p></li>
<li><p>VinUniversity</p></li>
</ul>
<p>详细讨论了三种点云距离度量方式: Chamfer divergence ，EMD，
还有自己提的sliced Wasserstein distance。</p>
<p><strong>Chamfer divergence</strong>:</p>
<p><span class="math display">\[
d_{\mathrm{CD}}(P, Q)=\frac{1}{|P|} \sum_{x \in P} \min _{y \in
Q}\|x-y\|_2^2+\frac{1}{|Q|} \sum_{y \in Q} \min _{x \in P}\|x-y\|_2^2
\]</span></p>
<p>大部分之前的工作喜欢用CD距离，因为其效果还不错，且计算简单。由于min操作的存在，对于每一个点来说，CD距离只关心其最近的一个邻域点，而不是其邻域分布。因此容易造成明明两个点的邻域分布差异大，但CD距离依然很小的情况。计算代价小是CD的优势，其最多能在<span
class="math inline">\(max{|P|,|Q|}\)</span>的数量级。</p>
<p><strong>EMD</strong>:</p>
<p><span class="math display">\[
d_{\mathrm{EMD}}(P, Q)=\min _{T: P \rightarrow Q} \sum_{x \in
P}\|x-T(x)\|_2
\]</span></p>
<p>EMD属于一种Wasserstein
distance。根据一些工作以及定理，证明EMD的效果比CD更好，EMD距离越小总能使CD距离更小，但是反过来不成立。不过EMD的计算代价会比CD昂贵很多，大致在<span
class="math inline">\(max{|P|,|Q|}^3\)</span>的数量级，即使是最快的一种近似版本也在<span
class="math inline">\(max{|P|,|Q|}^2\)</span>的数量级，依然很昂贵。因此实际上工作中还是更喜欢使用CD距离，而本文作者也基于这种考虑，研究了一种计算接近CD距离，但是效果等价于EMD的度量方式。</p>
<p><strong>Sliced Wasserstein disatance</strong>:</p>
<p><span class="math display">\[
S W_p(\mu, \nu) \approx\left(\frac{1}{N} \sum_{i=1}^N
W_p^p\left(\pi_{\theta_i} \sharp \mu, \pi_{\theta_i} \sharp
\nu\right)\right)^{\frac{1}{p}} .
\]</span></p>
<p>为了节省计算开销，可以先将度量函数投影到某一个方向上，再去计算WD距离。SWD即将所有方向的距离单独计算完之后，再取平均。另外还有根据偏差计算的自适应SWD来自动选择投影方向数量。</p>
<figure>
<img src="https://i.imgur.com/AsMxFcH.png" alt="计算代价对比" />
<figcaption aria-hidden="true">计算代价对比</figcaption>
</figure>
<figure>
<img src="https://i.imgur.com/Qj5RKxE.png"
alt="用于点云重建时的性能对比" />
<figcaption aria-hidden="true">用于点云重建时的性能对比</figcaption>
</figure>
<h2 id="点云时间学习">点云时间学习</h2>
<h3 id="flownet3d">FlowNet3D</h3>
<ul>
<li>Liu X, Qi C R, Guibas L J. Flownet3d: Learning scene flow in 3d
point clouds[C]//Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 2019: 529-537.</li>
</ul>
<p>提出了flow embedding层，<strong>点对集合</strong>的matching
cost。其先通过<strong>ball
query</strong>找到邻域，然后计算邻域每个点对中心点的matching
cost，并且使用max pooling进行邻域聚合。</p>
<blockquote>
<p>这种聚合的坏处就是会丢失一些运动信息。</p>
</blockquote>
<h3 id="meteornet">MeteorNet</h3>
<ul>
<li>Liu X, Yan M, Bohg J. Meteornet: Deep learning on dynamic 3d point
cloud sequences[C]//Proceedings of the IEEE/CVF International Conference
on Computer Vision. 2019: 9246-9255.</li>
<li>卡耐基梅隆</li>
</ul>
<p>MeteorNet率先基于原始点云序列进行特征学习。由于点云的不规则性,
其不存在帧与帧之间点的一一对应,
因此也难以确定帧间点与点之间的时间联系。因此MeteorNet提出了两种聚类方法
<strong>Direct grouping</strong>和 <strong>Chained-flow
grouping</strong>来进行时间聚类。</p>
<blockquote>
<p>由于其需要显式的时空邻居, 这不利于提高准确率和泛化网络。</p>
</blockquote>
<h3 id="minkowskinet">MinkowskiNet</h3>
<ul>
<li>Choy C, Gwak J Y, Savarese S. 4d spatio-temporal convnets: Minkowski
convolutional neural networks[C]//Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. 2019: 3075-3084.</li>
</ul>
<p>针对点云的稀疏性, 提出了高效的时空4D CNN。</p>
<blockquote>
<p>但是既没有对时间规范化,
也不能进行时空特征聚合。计算代价昂贵。存在体素化的量化误差</p>
</blockquote>
<h3 id="caspr">CaSPR</h3>
<ul>
<li>Rempe D, Birdal T, Zhao Y, et al. Caspr: Learning canonical
spatiotemporal point cloud representations[J]. Advances in neural
information processing systems, 2020, 33: 13688-13701.</li>
<li>Stanford</li>
</ul>
<p>过去有一些工作做了动态点云的时间学习, 然而这些工作有一个致命限制:
它们缺少时间连续性、鲁棒性、同类泛化性。有一些工作考虑了其中某一个方面,
但没有对这三者整体进行统一的要求。</p>
<p>Canonical Spatiotemporal Point Cloud Representations
(CaSPR)致力于对3D形状的时空变化进行编码。</p>
<ol type="1">
<li>将输入的点云序列规范化到一个共享的4D container空间:
其先构建了坐标空间Normalized Object Coordinate Space (NOCS),
它能把同类中的一些外在属性引如位置、朝向和放缩程度给规范化。进一步的,
CaSPR将NOCS扩展到4D <strong>Temporal-NOCS(T-NOCS)</strong>,
额外将点云序列的持续时间归一化成一个单位时间。对于给定的点云序列,
最终规范化后会给出在<strong>时间和空间</strong>上都规范化的点云。</li>
<li>然后在规范化空间中学习连续的时空特征: 其使用了Neural Ordinary
Differential Equations (Neural ODEs)。</li>
</ol>
<h3
id="pointlstm-an-efficient-pointlstm-for-point-clouds-based-gesture-recognition">PointLSTM:
An Efficient PointLSTM for Point Clouds Based Gesture Recognition</h3>
<ul>
<li>Min Y, Zhang Y, Chai X, et al. An efficient pointlstm for point
clouds based gesture recognition[C]//Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 2020:
5761-5770.</li>
<li>中科院计算所</li>
</ul>
<p>之前的工作从时空领域中抽取运动特征和结构特征。然而这些工作仅局限于短期模型，缺乏捕捉长期联系的能力。PointLSTM通过在点云上构建LSTM模型来学习点云序列的长期联系。然而点云数据是无序的，因此直接在没有对齐的点云序列上应用一个权重共享的LSTM层会有更新困难的问题。因此，<strong>如何在保持空间结构的前提下利用时间信息就是主要的挑战</strong>。</p>
<p>PointLSTM对于每帧每个点都计算隐状态, 并且对于第t帧的点,
会在第t-1帧中找到其局部邻域所有点,
并且结合它们的LSTM隐状态来更新第t帧中心点。</p>
<p>简化版本PointLSTM-PSS将过去t-1帧整个点云视为一个隐状态,
并且对于第t帧的每个点都会利用这个隐状态进行更新。</p>
<p>另外也提出了一种基于密度采样点云的方法。</p>
<h3 id="pstnet">PSTNet</h3>
<ul>
<li>Fan H, Yu X, Ding Y, et al. PSTNet: Point spatio-temporal
convolution on point cloud sequences[C]//International Conference on
Learning Representations. 2020.</li>
<li>新加坡国立大学</li>
</ul>
<p>在聚合时间邻域上提出了<strong>Point tube</strong>的结构,
对前后相邻帧的点进行时间聚类。另外在邻域定义的基础上,
由于点云的不规则性,
传统规则卷积无法计算连续变化的点云坐标差。因此提出了<strong>PSTConv</strong>稀疏4D卷积模块。其将卷积定义为根据偏移量计算权重的连续核函数。</p>
<h3
id="sequentialpointnet-a-strong-parallelized-point-cloud-sequence-network-for-3d-action-recognition">SequentialPointNet:
A strong parallelized point cloud sequence network for 3D action
recognition</h3>
<ul>
<li>Li X, Huang Q, Wang Z, et al. SequentialPointNet: A strong
parallelized point cloud sequence network for 3D action recognition[J].
arXiv preprint arXiv:2111.08492, 2021.</li>
<li>河海大学计算机</li>
</ul>
<p>针对人类动作在空间上复杂，在时间上简单的特性，不平等的对待空间信息和时间信息。提出了一个强并行能力的点云序列网络SequentialPointNet：一个帧内appearance编码模块，一个帧间动作编码模块。</p>
<ul>
<li>为了对人体动作丰富的空间信息建模，每帧先在帧内的appearance
encoding中并行处理，并且输出一个特征向量序列，描述静态的appearance在时间维度上的改变。</li>
<li>为了建模简单的时间维度上的变化，在帧间的动作编码模块中，在特征向量序列中应用了
时间上的位置编码和分层的池化策略。</li>
<li>为了更好的挖掘时空内容，聚合人体动作的多级特征。</li>
</ul>
<h3 id="point4dtransformer">Point4DTransformer</h3>
<ul>
<li>Fan H, Yang Y, Kankanhalli M. Point 4D transformer networks for
spatio-temporal modeling in point cloud videos[C]//Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021:
14204-14213.</li>
<li>新加坡国立大学/悉尼科技大学</li>
</ul>
<p>在PSTNet的PSTConv卷积提取局部特征的基础上,
将各个局部特征连接到一个Transformer层进行权重提取。</p>
<p>其中位置编码使用了一维卷积来实现</p>
<h3 id="pst-transformer">PST-Transformer</h3>
<ul>
<li>Fan H, Yang Y, Kankanhalli M. Point Spatio-Temporal Transformer
Networks for Point Cloud Video Modeling[J]. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2022.</li>
<li>JCR 一区</li>
<li>新加坡国立大学</li>
</ul>
<p>相比于P4T的全局注意力搜索，和PST的局部建模能力，PST-Transformer集合了两者，进行时空邻域的建模。</p>
<figure>
<img src="https://i.imgur.com/hRhgDLA.png" alt="系列对比" />
<figcaption aria-hidden="true">系列对比</figcaption>
</figure>
<p>首先，通过一个video-level的自注意力进行帧加权。对于<strong>frame-level</strong>，仅对一帧进行注意力计算，由于点的流动性，可能会损失较多轨迹信息。而<strong>video-level</strong>，对两个查询帧之间的所有帧进行注意力计算，更适合保留时空信息。</p>
<p><span class="math display">\[
\alpha_{p p^{\prime}}=\frac{e^{A_{p p^{\prime}}}}{\sum_{t^{\prime
\prime}=1}^{L} \sum_{p^{\prime \prime} \in P_{t^{\prime \prime}}}
e^{A_{p p^{\prime \prime}}}}
\]</span></p>
<p>其次通过一个和PST一样的point 4D conv进行时空编码。</p>
<figure>
<img src="https://i.imgur.com/CSh1yZu.png" alt="PST-T整体结构图" />
<figcaption aria-hidden="true">PST-T整体结构图</figcaption>
</figure>
<h3 id="geometrymotion-net">GeometryMotion-Net</h3>
<ul>
<li>Liu J, Xu D. GeometryMotion-Net: A strong two-stream baseline for 3D
action recognition[J]. IEEE Transactions on Circuits and Systems for
Video Technology, 2021, 31(12): 4711-4721.</li>
<li>北航计算机</li>
<li>中科院二区</li>
</ul>
<p>GeometryMotion-Net用于在点云序列中抽取几何和运动信息，并且不依赖于任何体素化操作。主要思想是利用一个<strong>几何流</strong>和<strong>运动流</strong>组成的two-stream框架来进行动作识别。</p>
<p><strong>几何流</strong>: 将所有帧点云合并成一个大点云,
再进行传统的空间点云处理, 如PointNet++。</p>
<p><strong>运动流</strong>:
在所有帧之间插值计算出一个关于运动变化的虚拟帧。再在这些虚拟帧上进行空间点云处理,
得到一组特征。</p>
<p><strong>双流汇聚</strong>: 将一个几何流的特征和 N
个运动流的特征拼接合并输出。</p>
<h3 id="tranquil-clouds">Tranquil clouds</h3>
<ul>
<li>Prantl L, Chentanez N, Jeschke S, et al. Tranquil Clouds: Neural
Networks for Learning Temporally Coherent Features in Point
Clouds[C]//International Conference on Learning Representations.
2020.</li>
<li>慕尼黑工业大学</li>
</ul>
<p>基于推土机距离Earth Mover’s Distance (EMD)提出了一个新的损失函数,
用于衡量两个点云之间的差异性:</p>
<p><span class="math display">\[
\mathcal{L}_{S}=\min _{\phi: \tilde{y} \rightarrow y}
\sum_{\tilde{y}_{i} \in
\tilde{Y}}\left\|\tilde{y}_{i}-\phi\left(\tilde{y}_{i}\right)\right\|_{2}^{2}
\]</span></p>
<h3 id="pointpwc-net">PointPWC-Net</h3>
<ul>
<li>Wu W, Wang Z Y, Li Z, et al. Pointpwc-net: Cost volume on point
clouds for (self-) supervised scene flow estimation[C]//European
conference on computer vision. Springer, Cham, 2020: 88-107.</li>
<li>Oregon State University</li>
</ul>
<p>提出的<strong>可学习的相继两个点云的matching cost</strong>：找到<span
class="math inline">\(p_t^j\)</span>在上一帧中的邻域，并且计算邻域所有点与其的特征差和坐标差。</p>
<blockquote>
<p>这种<strong>点对点</strong>的matching cost对异常点特别敏感。</p>
</blockquote>
<h3 id="spcm-net">SPCM-Net</h3>
<ul>
<li>He P, Emami P, Ranka S, et al. Learning Scene Dynamics from Point
Cloud Sequences[J]. International Journal of Computer Vision, 2022:
1-27.</li>
<li>Q1 CCF-A</li>
<li>University of Florida ，CS</li>
</ul>
<p>主要是做序列点云的场景流估计以及预测任务。之前的场景流估计一般都是t-1帧预测t帧，两帧之间的联系。本文定义了序列多帧联系的场景流估计问题。并且基于这个问题，提出了一些序列学习的方法。</p>
<ul>
<li>Intra-Frame Feature Pyramid
(IFFP)：依照了PointPWC-Net的结构，由于不能直接对点云进行传统卷积，使用了PointConv层进行卷积处理。<strong>并且通过多次FPS采样卷积中心，构建了多个金字塔式特征</strong>。</li>
<li>Inter-Frame Spatiotemporal Correlation (IFSC):
为了能找到时空联系，很自然我们希望使时间维度上的receptive
field能够尽可能覆盖到整个序列。因此借鉴了传统序列模型的<strong>LSTM结构</strong>,使用了一个
<strong>recurrent cost volume</strong>
结构来保存一定的时间信息。并且针对matching
cost，提出了和PointPWC以及FlowNet不同的
<strong>集合对集合</strong>的maching cost。</li>
<li>Multi-scale Coarse-to-Fine Prediction: 两个帧的特征+cost
volume的特征生成最低级(粗粒度)的预测点，然后通过Pointnet++的特征上采样传播逐渐生成细粒度特征。</li>
</ul>
<h3 id="pstt">PSTT</h3>
<ul>
<li>Wei Y, Liu H, Xie T, et al. Spatial-Temporal Transformer for 3D
Point Cloud Sequences[C]//Proceedings of the IEEE/CVF Winter Conference
on Applications of Computer Vision. 2022: 1171-1180.</li>
<li>中山大学</li>
<li>不在CCF h5指数62 排计算机视觉第12</li>
</ul>
<p>提出了<strong>Spatio-Temporal
Self-Attention(STSA)</strong>模块和<strong>Resolution
Embedding(RE)</strong>模块。STSA用于时空联系，RE用于聚合邻域特征，增强特征图的分辨率。</p>
<p>现有的基于point的时空方法要么是使用注意力机制，要么是使用RNN模型。然而，这些方法依赖于长期联系，导致信息冗余。STSA使用了自注意力来提取帧间联系。这样会使冗余程度下降，鲁棒性提高(残差+layer
normalization)，训练速度提升。</p>
<p>另外，在语义分割上面的编码器-解码器结构，在编码器降维时会造成信息丢失。RE模块使用了注意力权重来加强分辨率。</p>
<h3 id="hyperpointnet">HyperPointnet</h3>
<ul>
<li>Li X, Huang Q, Yang T, et al. Hyperpointnet for Point Cloud
Sequence-Based 3D Human Action Recognition[C]//2022 IEEE International
Conference on Multimedia and Expo (ICME). IEEE, 2022: 1-6.</li>
<li>河海大学</li>
<li>CCF B 会议</li>
</ul>
<p>将PointNet扩展到时间序列。作者提认为基于时空局部结构的点云序列模型会导致昂贵的计算和推理误差，因此其构造了<strong>HyperPoint</strong>的概念...本质上就是一个串行的帧内帧间编码网络。</p>
<figure>
<img src="https://i.imgur.com/fcsguio.png" alt="HyperPoint" />
<figcaption aria-hidden="true">HyperPoint</figcaption>
</figure>
<p><strong>帧内处理</strong>：通过经典的sample and group
层进行空间聚合学习，然后通过MLP进行维度变换。值得一提的是，其在每个group层，将邻域点的距离视为一个额外的特征维度。并且在group之后，使用了一个inter-feature的注意力模块CBAM来对特征进行重新整合。</p>
<p><strong>帧间处理</strong>：
每帧在帧内处理之后变成了所谓的一个HyperPoint，也就是逐帧的空间编码特征而已。其强调HyperPoint的主要信息来源于其内部结构，而不是HyperPoint之间。然后，给这个HyperPoint序列增加了一个Transformer的三角位置编码，并且再使用了frame-level的Pointnet进行最后的时间特征整合。</p>
<h3 id="virtualactionnet">VirtualActionNet</h3>
<ul>
<li>Li X, Huang Q, Wang Z, et al. VirtualActionNet: A strong two-stream
point cloud sequence network for human action recognition[J]. Journal of
Visual Communication and Image Representation, 2022: 103641.</li>
<li>河海大学</li>
<li>JCR 3区</li>
</ul>
<p>延续HyperPoint的工作，延展到Two-Stream框架。</p>
<figure>
<img src="https://i.imgur.com/aGKzR83.png" alt="Appearance Stream" />
<figcaption aria-hidden="true">Appearance Stream</figcaption>
</figure>
<p>其中注意在空间特征中附加了一个time
stamp特征，并且使用feature-level的自注意力进行特征混合。</p>
<p>为了补充appearance中的缺少的运动信息，构建了一个motion
stream，其中用了GRU来构建时间联系。</p>
<p>另外，其在训练函数里添加了一个Two-Stream的<strong>相似性约束</strong>
<span class="math inline">\(M_i \log{A_i}\)</span>
，以引导两个stream对同一个动作<span
class="math inline">\(i\)</span>的预测结果<span
class="math inline">\(M_i,A_i\)</span>一致，这在NTU 60上提高了<span
class="math inline">\(\%1.9\)</span>的准确率。</p>
<h3 id="action-recognition-from-silhouette-sequences">Action recognition
from silhouette sequences</h3>
<ul>
<li>Rusu R B, Bandouch J, Marton Z C, et al. Action recognition in
intelligent environments using point cloud features extracted from
silhouette sequences[C]//RO-MAN 2008-The 17th IEEE International
Symposium on Robot and Human Interactive Communication. IEEE, 2008:
267-272.</li>
<li>慕尼黑工业大學</li>
</ul>
<p>从图像序列构建3D点云，进而抽取特征实现动作识别。</p>
<p>从投影图像序列中通过marching
cubes算法构建代表时空数据的三维点云。</p>
<figure>
<img src="https://i.imgur.com/Kr9tDFu.png" alt="图像序列构建时空数据" />
<figcaption aria-hidden="true">图像序列构建时空数据</figcaption>
</figure>
<p>并且通过剪枝算法来尽可能过滤掉变化不大的帧，以便对动作的速度保证不变性，其过滤效率为24.15%~91.47%：</p>
<ol type="1">
<li>对于两个相邻帧，计算每一个点与其在相邻帧的K近邻点的欧拉距离。</li>
<li>对上一步构建的距离矩阵计算Frobenius
Norm矩阵范数，即得到帧距离。</li>
<li>通过人为阈值0.2对帧距离进行过滤。</li>
</ol>
<figure>
<img src="https://i.imgur.com/502jS7L.png" alt="红色为过滤帧" />
<figcaption aria-hidden="true">红色为过滤帧</figcaption>
</figure>
<p>最后通过PCA进行连续法线估计，并且通过法线构建形状直方图进行特征分类。</p>
<h3 id="facial-action-analysis">Facial action analysis</h3>
<ul>
<li>Reale M J, Klinghoffer B, Church M, et al. Facial action unit
analysis through 3d point cloud neural networks[C]//2019 14th IEEE
International Conference on Automatic Face &amp; Gesture Recognition (FG
2019). IEEE, 2019: 1-8.</li>
<li>CCF C</li>
</ul>
<p>第一次使用了通过点云网络进行面部表情分析：PointNet、PCCN、LCPN (new)
。论文又翻译了一遍前两者的网络结构，然后综合成了第三个自己提出的网络。</p>
<figure>
<img src="https://i.imgur.com/RH31S9k.png"
alt="PCCN Layer用于学习局部结构" />
<figcaption aria-hidden="true">PCCN Layer用于学习局部结构</figcaption>
</figure>
<figure>
<img src="https://i.imgur.com/b4YiNR1.png"
alt="LCPN 结构：综合Pointnet的全局学习能力和PCCN的局部学习能力" />
<figcaption aria-hidden="true">LCPN
结构：综合Pointnet的全局学习能力和PCCN的局部学习能力</figcaption>
</figure>
<h3 id="asta3dcnns">ASTA3DCNNs</h3>
<ul>
<li>Wang G, Liu H, Chen M, et al. Anchor-based spatio-temporal attention
3-D convolutional networks for dynamic 3-D point cloud sequences[J].
IEEE Transactions on Instrumentation and Measurement, 2021, 70:
1-11.</li>
<li>1区</li>
<li>上交</li>
</ul>
<p>anchor-baesd spatio-temporal attention 3D
convolutional网络，主要首先通过虚拟的anchor构建规则的邻域区域，再通过ASTA3DConv离散卷积算子进行特征聚合，本质上是一种两级两种形式的set
abstraction层。</p>
<figure>
<img src="https://i.imgur.com/mLky44z.png"
alt="Anchor-based两级局部结构：先从中心点延伸出正四面体的四个顶点，再在四个顶点上构建ball query的局部邻域" />
<figcaption
aria-hidden="true">Anchor-based两级局部结构：先从中心点延伸出正四面体的四个顶点，再在四个顶点上构建ball
query的局部邻域</figcaption>
</figure>
<p>后续就是一些网络操作，注意力嵌入层构建之类的。最终在MSR
Action3D上达到93.03%。</p>
<h3 id="aspnet">ASPNet</h3>
<ul>
<li>Liu J, Guo J, Xu D. Apsnet: Toward adaptive point sampling for
efficient 3d action recognition[J]. IEEE Transactions on Image
Processing, 2022, 31: 5287-5302.</li>
<li>Q1</li>
<li>北航</li>
</ul>
<p>主要贡献 ASPNet
，用于对不同的点云视频，决定不同大小的采样密度，以提升处理效率。</p>
<figure>
<img src="https://i.imgur.com/IfVisYX.png" alt="ASPNet core module" />
<figcaption aria-hidden="true">ASPNet core module</figcaption>
</figure>
<p>ASPNet本质上是一个采样密度决定器，特征提取网络还是依靠MLP和PointNet等经典网络作为BackBone(图中BBNet)。主要结构分为两部分：</p>
<ol type="1">
<li>Decision making stage：本质上是一个mini
LSTM特征分类模块，分类目标是四种采样密度。注意最后max计算输出label时使用的是Gumbel-Max。</li>
<li>Fine feature extraction
stage：网络首先预训练好了四种密度的BBNet，然后构建了一个选择分支网络，通过DM模块输出的密度label，来选择合适密度的BBNet。</li>
</ol>
<p>在损失函数处将计算效率FLOPs也算入损失，以引导网络选择合适的分支 <span
class="math inline">\(L=L_{acc} \dot L_{eff}\)</span>。</p>
<h3 id="pointmotionnet">PointMotionNet</h3>
<ul>
<li>Wang J, Li X, Sullivan A, et al. PointMotionNet: Point-Wise Motion
Learning for Large-Scale LiDAR Point Clouds Sequences[C]//Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
2022: 4419-4428.</li>
<li>CCF A</li>
<li>University of Maryland</li>
</ul>
<p>一种时空局部结构的卷积方法Point-STC，两层结构，先确定<strong>中心点</strong><span
class="math inline">\(\mathbf{p}_i^{(t)}\)</span>，然后中心点扩展出规则的<strong>代理点</strong><span
class="math inline">\(\mathcal{K}\left(\mathbf{p}_i^{(t)}\right)=\left\{\mathbf{p}_i^{(t)}-\mathbf{b}_k
\in
\mathbb{R}^3\right\}_{k=1}^K\)</span>，然后再通过代理点去加权<strong>邻域点</strong>。</p>
<figure>
<img src="https://i.imgur.com/dLwfRQ3.png" alt="Point-STC" />
<figcaption aria-hidden="true">Point-STC</figcaption>
</figure>
<p>其最后的局部抽取规则由<strong>全局权重</strong><span
class="math inline">\(h\)</span>和<strong>邻域点关联权重</strong><span
class="math inline">\(w\)</span> 相乘而得。</p>
<p><span class="math display">\[
\Psi_{\mathbf{p}_i^{(t)},
\mathbf{p}_j^{(t+\tau)}}\left(\mathbf{x}_j^{(t+\tau)}\right)=\sum_{\delta_k
\in \mathcal{K}\left(\mathbf{p}_i^{(t)}\right)} w_k h\left(\delta_k,
\mathbf{p}_j^{(t+\tau)}\right) \mathbf{x}_j^{(t+\tau)}
\]</span></p>
<p><span class="math inline">\(w\)</span>为所有点共享的可学习权重，<span
class="math inline">\(h\)</span>为每个局部邻域独有的核函数：</p>
<p><span class="math display">\[
h\left(\delta_k, \mathbf{p}_j^{(t+\tau)}\right)=\max
\left(0,1-\frac{\left\|\delta_k-\mathbf{p}_j^{(t+\tau)}\right\|_2}{\sigma}\right)
\in \mathbb{R}
\]</span></p>
<p>另外也使用了多层金字塔结构来帮助卷积扩大感受野。</p>
<figure>
<img src="https://i.imgur.com/sCYRIUn.png" alt="pyramid structure" />
<figcaption aria-hidden="true">pyramid structure</figcaption>
</figure>
<h3 id="geometrymotion-transformer">GeometryMotion-Transformer</h3>
<ul>
<li>Liu J, Guo J, Xu D. GeometryMotion-Transformer: An End-to-End
Framework for 3D Action Recognition[J]. IEEE Transactions on Multimedia,
2022.</li>
<li>一区</li>
<li>北航</li>
</ul>
<p>feature extraction Module(FEM)模块，核心在于构建一个motion
point，用于聚合时空信息。</p>
<figure>
<img src="https://i.imgur.com/s3Mi1Q7.png"
alt="FEM模块: 注意里面的motion extraction module是多尺度整合的" />
<figcaption aria-hidden="true">FEM模块: 注意里面的motion extraction
module是多尺度整合的</figcaption>
</figure>
<p>注意FEM里面的Motion extraction
module。其同样也是在相邻帧中通过KNN寻找邻域点，但是gather方法有点不一样。其通过邻域点和中心点的
特征向量余弦相似度 和 距离 的乘积来衡量影响权重<span
class="math inline">\(w_{i,j}o_{i,j}\)</span>，并最终确定一个新的motino
point的坐标<span class="math inline">\(r_i\)</span>:</p>
<p><span class="math display">\[
s_{i, j}=\frac{\mathbf{g}_i^c \cdot
\mathbf{g}_j^r}{\left\|\mathbf{g}_i^c\right\|
\cdot\left\|\mathbf{g}_j^r\right\|}\\
\mathbf{r}_i=\sum_{j=1}^K w_{i, j} \mathbf{o}_{i, j}, \text { where }
w_{i, j}=\frac{\exp \left(s_{i, j} / \tau\right)}{\sum_{j=1}^K \exp
\left(s_{i, j} / \tau\right)}
\]</span></p>
<p><span class="math inline">\(r_{i,j}\)</span>结合相似度最高的特征<span
class="math inline">\(z_i\)</span>即构成一个完整的新特征点，并进入后续的MLP等特征处理变换中。</p>
<p>FEM模块最终可以得到空间特征<span class="math inline">\(Q,
G\)</span>以及运动特征<span
class="math inline">\(M\)</span>。接下来的问题是怎么整合这三者，于是有了Feature
fusion module(FFM)的注意力特征融合:</p>
<figure>
<img src="https://i.imgur.com/gTXIINx.png" alt="FFM模块" />
<figcaption aria-hidden="true">FFM模块</figcaption>
</figure>
<h3
id="continuous-body-and-hand-gesture-recognition-for-natural-human-computer-interaction">Continuous
Body and Hand Gesture Recognition for Natural Human-Computer
Interaction</h3>
<ul>
<li>Song Y, Demirdjian D, Davis R. Continuous body and hand gesture
recognition for natural human-computer interaction[J]. ACM Transactions
on Interactive Intelligent Systems (TiiS), 2012, 2(1): 1-28.</li>
<li>3区</li>
<li>MIT CS</li>
</ul>
<p>值得参考其如何构建连续识别模型。连续识别模型目的在于输出一个序列中每一帧的label，而不仅仅是这个序列给同样的label。</p>
<figure>
<img src="https://i.imgur.com/UJetjfg.png" alt="滑动窗口示意图" />
<figcaption aria-hidden="true">滑动窗口示意图</figcaption>
</figure>
<p>在窗口内部，其使用高斯函数来整合相邻帧的特征向量:</p>
<p><span class="math display">\[
g(w)[n]=e^{-\frac{1}{2}\left(\alpha \frac{n}{w / 2}\right)^2}
\]</span></p>
<p>在窗口外部以两级形式输出单帧label:</p>
<ul>
<li>local level:
即对于同一帧，在长度为K的滑动窗口经过的step里，每一次都在窗口内进行一次整合计算。并最终平均整合K次计算的local
feature。</li>
<li>global level: 通过指数衰减的方式平滑整合local
level和随时间变化的global level:</li>
</ul>
<p><span class="math display">\[
q_t\left(y_j\right)=\alpha \cdot \bar{p}_t\left(y_j\right)+(1-\alpha)
\cdot q_{t-1}\left(y_{j-1}\right), where \alpha = max
\bar{p}_t\left(y_j\right)
\]</span></p>
<p>对于连续动作的边界判断来说，global level 变化的地方即是边界。</p>
<h2 id="基于其他三维数据">基于其他三维数据</h2>
<h3 id="dgcnn">DGCNN</h3>
<ul>
<li>Wang Y, Sun Y, Liu Z, et al. Dynamic graph cnn for learning on point
clouds[J]. Acm Transactions On Graphics (tog), 2019, 38(5): 1-12.</li>
</ul>
<h3
id="ddgcn-a-dynamic-directed-graph-convolutional-network-for-action-recognition">DDGCN:
A Dynamic Directed Graph Convolutional Network for Action
Recognition</h3>
<ul>
<li>Korban M, Li X. Ddgcn: A dynamic directed graph convolutional
network for action recognition[C]//European Conference on Computer
Vision. Springer, Cham, 2020: 761-776.</li>
<li>University of Virginia</li>
</ul>
<p>DDGCN认为骨架的空间层级结构和动作的时间序列结构都包含了顺序信息，然而大多数ST
graph都是用了无向图结构，即无视了顺序信息，因此DDGCN提出了<strong>有向图骨架结构
Directed Spatial-Temporal Graph (DSTG)</strong>
。通过有向图中父子节点的定义，父节点的动作实际上会影响到子节点的动作，因此DDGCN在有向图的基础上定义了
bone features来表示父子节点的影响特征。</p>
<p>另外针对图卷积的邻域不确定性，DDGCN提出<strong>Dynamic Convolutional
Sampling (DCS)</strong>
来对一个节点的邻居列表进行动态的排序，形成动态邻域关系。然而卷积核的权重是有顺序的，而邻居列表是动态变化的，可能会造成权重分配的错序。因此DDGCN使用了一个<strong>Dynamic
Convolutional Weights
(DCW)</strong>模块来对邻居列表和权重列表进行一个<strong>Dynamic Time
Warping (DTW)距离</strong>的最小化排序，再根据这个排序进行权重分配。</p>
<h3 id="skeletontransformer">SkeletonTransformer</h3>
<ul>
<li>Plizzari C, Cannici M, Matteucci M. Skeleton-based action
recognition via spatial and temporal transformer networks[J]. Computer
Vision and Image Understanding, 2021, 208: 103219.</li>
<li>Politecnico di Torino 意大利都灵理工大学</li>
<li>三区</li>
</ul>
<p>空间Transformer: <strong>Spatial Self-Attention
(SSA)</strong>模块，用于在骨架之间动态的建立联系，而独立于人体真实骨架结构。
时间Transformer: <strong>Temporal Self-Attention
(TSA)</strong>模块用于学习关节在时间上的变化。</p>
<p>值得注意的是, 其空间时间的Transformer不是串行计算,
而是使用<strong>Two-Stream</strong>方法分为两条管线独立运算。最终再对两个管线输出特征进行拼接处理。</p>
<h3 id="dv">3DV</h3>
<ul>
<li>Wang Y, Xiao Y, Xiong F, et al. 3dv: 3d dynamic voxel for action
recognition in depth video[C]//Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition. 2020: 511-520.</li>
<li>华中科技大学</li>
</ul>
<p>3DV通过对点云视频进行体素化, 提取出3D动态体素的表示。</p>
<blockquote>
<p>体素化的问题: 体素是是计算消耗巨大的过程, 时间和空间距离相同不太可取,
时间戳本身会影响效果。</p>
</blockquote>
<h3 id="histogram-of-motion-trajectory-feature">Histogram of motion
trajectory feature</h3>
<ul>
<li>Li D, Jahan H, Huang X, et al. Human action recognition method based
on historical point cloud trajectory characteristics[J]. The Visual
Computer, 2022, 38(8): 2971-2979.</li>
<li>四川大学 CS</li>
<li>中科院JCR 3区</li>
</ul>
<p><strong>时间金字塔</strong>：即逐层分割时间序列，二分、四分......解释上一方面可以有助于识别不同长度的动作特征，另一方面，时间的片段化也能强化顺序信息。</p>
<p><strong>分割四肢</strong>：通过kinect的骨骼坐标定位四肢点云并分割。</p>
<p><strong>3D网格划分点云</strong>：将点云空间划分为 <span
class="math inline">\(W \times H \times d\)</span>
的网格空间以形成3D直方图。每个网格内的点云数量归一化到<span
class="math inline">\([y_{min},y_{max}]\)</span>。</p>
<figure>
<img src="https://i.imgur.com/20tjGyc.png" alt="3D网格划分" />
<figcaption aria-hidden="true">3D网格划分</figcaption>
</figure>
<p><span class="math display">\[
\mathrm{HOMT}_{w i, h i, d i}=y_{\min }+\frac{\left(y_{\max }-y_{\min
}\right)\left(num_{w i, h i, d i}-\operatorname{Min}(n u
m)\right)}{\operatorname{Max}(\text { num })-\operatorname{Min}(\text {
num })}
\]</span></p>
<p>最终以3D直方图特征<span class="math inline">\(HOMT \in \mathbb{R}^{t
\times w \times h \times
d}\)</span>作为特征描述子，再通过支持向量机进行特征分类。</p>
<p>最终在UTD-MHAD上的分类结果到90.23%，不如SOTA
91.13%，声称效率更高。</p>
<h3 id="d-rans">3D RANs</h3>
<ul>
<li>Cai J, Hu J. 3D RANs: 3D residual attention networks for action
recognition[J]. The Visual Computer, 2020, 36(6): 1261-1270.</li>
<li>中山大学</li>
</ul>
<figure>
<img src="https://i.imgur.com/6A31Qpn.png" alt="3D RAN" />
<figcaption aria-hidden="true">3D RAN</figcaption>
</figure>
<figure>
<img src="https://i.imgur.com/NE0XABU.png" alt="channel attention" />
<figcaption aria-hidden="true">channel attention</figcaption>
</figure>
<figure>
<img src="https://i.imgur.com/zXpHetx.png" alt="spatial attention" />
<figcaption aria-hidden="true">spatial attention</figcaption>
</figure>
<h3 id="fine-grained">Fine-grained</h3>
<ul>
<li>Zhu Y, Liu G. Fine-grained action recognition using multi-view
attentions[J]. The Visual Computer, 2020, 36(9): 1771-1781.</li>
<li>南京信息工程大学</li>
</ul>
<figure>
<img src="https://i.imgur.com/gtCaHFs.png" alt="CT CS ST attention" />
<figcaption aria-hidden="true">CT CS ST attention</figcaption>
</figure>
<h2 id="基础设施">基础设施</h2>
<h3 id="点云分类综述">点云分类综述</h3>
<p><strong>点云特征</strong>：</p>
<ol type="1">
<li><p>无序性：点云数据则是无序点的集合。使用不同的设备和位置获取采集目标，会得到排列顺序千差万别的点云数据。当采用不同顺序读入
n 个点云时，其组合方式就有
n！种。对不同位置点云进行卷积算，结果会受<strong>点云的输入顺</strong>序的影响。通过对称函数、构造卷积算子或利用图与树的结构为解决点云的无序性做出贡献。</p></li>
<li><p>稀疏性：通过不同方式获取到物体的点云数据在密度、点数以及点间距离都具有一定的差别。三维点云的不规则结构会导致某些区域的过采样和欠采样。因此不同密度的点云的处理是研究点云分类策略的重点之一。在网络中嵌入<strong>密度模块</strong>的方法可以在一定程度上解决点云密度不均的问题。</p></li>
<li><p>非结构化：无结构的点云数据直接输入到神经卷积网络模型中往往比较困难。早期有<strong>体素</strong>方法或者<strong>多视图</strong>方法。但是会增加大量的数据计算。近年有<strong>图卷积神经网络</strong>处理非结构化数据。如有基于<strong>Reeb图</strong>卷积神经网络聚合点云特征。</p></li>
</ol>
<p><strong>数据集</strong>：</p>
<ol type="1">
<li>ModelNet：3D CAD模型</li>
<li>ScanNet：RGBD视频，室内场景</li>
<li>ISPRS：城市目标，三维建筑</li>
<li>2019 Data FusionContest Dataset：城市场景</li>
</ol>
<p><strong>基于体素网格</strong>：</p>
<p>将环境状态表示为三维网格，借鉴二维图像的相似性。</p>
<ol type="1">
<li>VoxNet：集成了体积网格和3D卷积网络</li>
<li>3D
Shapenets：将点云特征表示为体素网格的二进制概率分布，卷积共享权值环节参数过剩。</li>
<li>OctNet、OCNN等：优化体素结构，使用灵活的八叉树结构。</li>
</ol>
<p>体素网格存在丢失重要信息，存储和计算开销大，适用性不高等弊端。</p>
<p><strong>基于多视角</strong>：</p>
<p>多个视角对点云投影，使用CNN对投影的2D影像进行加工。</p>
<ol type="1">
<li>MVCNN：多视图CNN，将3D渲染成传统图像。将多个视图的特征信息通过卷积层和池化层整合成单一的3D描述符。最后进入全连接层计算。</li>
<li>Qi在MVCNN基础上改进：通过变化仰角和方位角增强训练数据、引入三维滤波捕捉多尺度信息。</li>
<li>GVCNN：对不同视图的视觉描述符分组，学习组间特征生成组级别的描述符，再加权获得3D描述符。</li>
</ol>
<p>在相机设置位置与角度时容易出现遮挡情况，视图不能得到有效处理将直接影响训练结果。3D
到 2D 的转换过程中会造成点云信息的丢失。</p>
<p><strong>基于原始点云</strong>：</p>
<p>MLP</p>
<ol type="1">
<li>PointNet：通过MLP学习单个点的特征，利用对称函数编码全局信息<strong>解决无序性问题</strong>。利用空间变换网络STNs<strong>解决点云旋转不变性问题</strong>。对输入点云进行几何变换和特征变换，采用最大池化聚合点特征解决<strong>置换不变性问题</strong>。<br />
缺陷在于只捕捉到单个点和全局点，没有有效的局部特征信息，且没有点的邻近关系。导致对细粒度效果较差。<br />
</li>
<li>PointNet++：引入多层次结构。每一层分为采样层、分组层、特征提取层。解决了局部点云特征的问题，点间联系依然没有充分学习。</li>
<li>Momenet：对点云坐标使用多项式函数提高训练能力，高时效低消耗</li>
<li>So-Net：利用自组织特征映射SOFM分析点云分布，实现置换不变性网络。结构简单、训练速度快。分类效果良好。</li>
<li>SRN、PointWeb：学习点间局部联系。</li>
<li><strong>PointASNL</strong>：自适应采样AS减弱噪声和异常。局部-非局部模块L-NL提供准确稳定的特征信息。</li>
<li>BPS：针对无序性、提出点集概念。将输入点归一化，对一组点随机采样构成点集单元。</li>
</ol>
<p>CNN</p>
<ol type="1">
<li>PointCNN：避免点云输入顺序对卷积操作的影响。X-变换卷积算子将数据转换为顺序无关的特征。分类中使用膨胀卷积思想。证实了局部结构对点云的重要性。但变换算子仍效果不够好。</li>
<li>RSCNN：基于几何关系，几何关系编码卷子算子RS-Conv。有良好的目标识别功能。</li>
<li>DANCE-NET：密度感知卷积模块。</li>
</ol>
<p>GCN</p>
<ol type="1">
<li>GCN：提取图数据特征，在半监督分类任务中效果良好。</li>
<li>ECC：将点云视作图结构的顶点，聚合顶点信息转换为图结构，但是需要大量计算，不理想。</li>
<li>LDGCNN、PointGNN、PointVGG</li>
</ol>
<p>注意力机制</p>
<p>具有固定排列、不依赖于点间联系的特性。</p>
<ol type="1">
<li>GAPNet：在MLP层中嵌入图注意力机制学习局域点云语义。<strong>GAPLayer</strong>和<strong>注意力层</strong>可以嵌入到其他模型中以提取局部几何特征。</li>
<li>清华PCT：将点云编码至高维特征空间，经过四层注意力层（自注意力、offset注意力）获取局部几何信息。</li>
</ol>
<h3 id="deep-metric-learning-a-survey">deep metric learning a
survey</h3>
<ul>
<li>Kaya M, Bilge H Ş. Deep metric learning: A survey[J]. Symmetry,
2019, 11(9): 1066.</li>
<li>土耳其的錫爾特大學</li>
<li>2区</li>
</ul>
<p>度量学习概述。传统的度量学习需要采用各种核方法来将数据转换到一个特征空间中，然后进行合适的距离度量。传统方式的缺点很明显，对于很多数据我们难以找到一个合适的变换方法来进行特征降维，另一方面对于多样化的相似度计算也难以囊括：如文本语义相似度。借助于深度学习强大的抽象能力，近期的度量学习研究基本都是受Siamese和Triplet的深度度量网络启发，使用深度学习来代替传统的手工构造核函数。</p>
<figure>
<img src="https://i.imgur.com/ohfXXMp.png" alt="深度度量学习框架" />
<figcaption aria-hidden="true">深度度量学习框架</figcaption>
</figure>
<figure>
<img src="https://i.imgur.com/1OGOb3t.png" alt="深度度量学习效果示意" />
<figcaption aria-hidden="true">深度度量学习效果示意</figcaption>
</figure>
<p>深度度量学习Pipeline难点主要分为两部分：</p>
<ol type="1">
<li>样本选择/数据集分割</li>
</ol>
<p>虽然最朴实的数据集分割方式是随机比例划分，但是在度量学习中，一方面由于我们的正样本对<span
class="math inline">\(X\)</span>、<span
class="math inline">\(X+\)</span>通常有限，而相对的负样本对<span
class="math inline">\(X\)</span>、<span
class="math inline">\(X-\)</span>会多很多，那么对于三元组合<span
class="math inline">\(X\)</span>、<span
class="math inline">\(X+\)</span>、<span
class="math inline">\(X-\)</span>中，<span
class="math inline">\(X+\)</span>会重复很多很多次，容易导致一定程度上的过拟合。</p>
<p>另一方面，如果组合所有可能的情况去训练的话会有<span
class="math inline">\(O(n_{sample}^3)\)</span>的组合数量级，这在时间上是难以接受的。</p>
<p>而且，有些输入的pair的区分价值很小，会导致浪费计算效率，而寻找区分价值大的pair也能使网络在有限的训练中得到最好的测试效果。因此，业界提出了hard
、semi-hard、easy三个层次的数据划分。以同类的距离为标志，hard处于同类距离以内，但其实是不同类数据，semi-hard和easy则以同类范围以外的一个margin作为边界划分。</p>
<figure>
<img src="https://i.imgur.com/K8I1K4I.png"
alt="寻找困难负样本 hard negative mining" />
<figcaption aria-hidden="true">寻找困难负样本 hard negative
mining</figcaption>
</figure>
<ol start="2" type="1">
<li>损失函数设计</li>
</ol>
<p>由于度量网络和普通的识别网络任务目标不同，其希望把所有同类的距离尽可能弄小，然后所有异类的距离尽可能弄大，因此单纯的使用交叉熵之类的多分类损失函数不可行。这一部分已经有比较充足的发展：</p>
<figure>
<img src="https://i.imgur.com/RhcgMtJ.png"
alt="几种度量网络损失函数。ab是标准的目标设计。c希望在ab的基础上细分异类样本为：较为相似的异类、不相似的异类，加强区分效果。e认为一次只在n-1个异类中取一个负样本不合适，因此改为一次取n个输入，其中1个正样本，n-1个负样本。gf则是以cluster的视角来看待问题。其中f强调对类的重叠进行惩罚。h则是一个3倍版的Triplet Loss." />
<figcaption
aria-hidden="true">几种度量网络损失函数。ab是标准的目标设计。c希望在ab的基础上细分异类样本为：较为相似的异类、不相似的异类，加强区分效果。e认为一次只在n-1个异类中取一个负样本不合适，因此改为一次取n个输入，其中1个正样本，n-1个负样本。gf则是以cluster的视角来看待问题。其中f强调对类的重叠进行惩罚。h则是一个3倍版的Triplet
Loss.</figcaption>
</figure>
<h3 id="human-motion-analysis-metric">Human motion analysis metric</h3>
<ul>
<li>Coskun H, Tan D J, Conjeti S, et al. Human motion analysis with deep
metric learning[C]//Proceedings of the European Conference on Computer
Vision (ECCV). 2018: 667-683.</li>
<li>慕尼黑工业大学</li>
<li>ECCV</li>
</ul>
<p>讲述如何衡量动作序列的差异，以便于实现动作相似度衡量和分析。传统的人体动作度量方法有
L2距离误差度量，以及Dynamic Time Warping
(DTW)距离度量。然而传统的方法首先需要对齐两个动作序列，而这本身就是一件很难计算的事。</p>
<figure>
<img src="https://i.imgur.com/i9xSHaC.png" alt="动作判别" />
<figcaption aria-hidden="true">动作判别</figcaption>
</figure>
<p>因此现在的度量学习基本都是基于深度学习来做，通常会构建一个深度学习网络学习一个将动作序列数据映射到一个低维embedding的一个映射，然后在这个低维空间上进行标准的L2距离衡量：</p>
<p><span class="math display">\[
d(f(X), f(Y))=\|f(X)-f(Y)\|^2
\]</span></p>
<figure>
<img src="https://i.imgur.com/by9hU75.png" alt="模型示例" />
<figcaption aria-hidden="true">模型示例</figcaption>
</figure>
<p>所以关键在于怎么构建这个神经网络以学习到判别性的映射。比较主流的方案是参考孪生神经网络，构造pair型的输入以及特殊的判别损失函数：</p>
<p><span class="math display">\[
\mathcal{L}_{\text {contrastive }}=(r) \frac{1}{2} d+(1-r)
\frac{1}{2}\left[\max \left(0, \alpha_{\text {margin
}}-d\right)\right]^2
\]</span></p>
<p>其中<span class="math inline">\(d\)</span>为上面的L2距离计算，<span
class="math inline">\(\alpha\)</span>则是一个需要寻找的超参数，判别损失优化后的版本是Triplet:</p>
<p><span class="math display">\[
\mathcal{L}_{\text {triplet }}=\max
\left(0,\left\|f(X)-f\left(X^{+}\right)\right\|^2-\left\|f(X)-f\left(X^{-}\right)\right\|^2+\alpha_{\text
{margin }}\right)
\]</span></p>
<p>另外为了避开<span
class="math inline">\(\alpha\)</span>这个超参数，这篇论文提出了Neighbourhood
Components Analysis(NCA)方法来改造损失函数。</p>
<p><span class="math display">\[
\mathcal{L}_{\mathrm{NCA}}=\frac{\exp
\left(-\left\|f(X)-f\left(X^{+}\right)\right\|^2\right)}{\sum_{X-\epsilon
C} \exp \left(-\left\|f(X)-f\left(X^{-}\right)\right\|^2\right)}
\]</span></p>
<blockquote>
<p>NCA本质上是一种邻居投票方法。首先计算<span
class="math inline">\(X_i\)</span>的邻居<span
class="math inline">\(X_j\)</span>的分布概率<span
class="math inline">\(p_{ij}\)</span>为： <span class="math display">\[
\mathcal{L}_{\mathrm{NCA}}=\frac{\exp \left(-\left\|f(X_i)-f\left( X_j
\right)\right\|^2\right)}{\sum_{X_k \epsilon C} \exp
\left(-\left\|f(X_i)-f\left(X_k \right)\right\|^2\right)}\]</span>
有了<span
class="math inline">\(p_{ij}\)</span>之后，我们可以在邻域内采样，例如假如采样到<span
class="math inline">\(X_j\)</span>，那就认为当前的<span
class="math inline">\(X_i\)</span>的标签是<span
class="math inline">\(y_j\)</span>。而当<span
class="math inline">\(y_j=y_i\)</span>时，那么这种采样投票就是正确的，因此最终的采样投票正确概率为：
<span class="math display">\[p_i=\sum_{j \in C}p_ij\]</span>
其中C为所有投票正确的邻居，进而整个网络的目标就是让所有的<span
class="math inline">\(p_i\)</span>尽可能的大，即： <span
class="math display">\[p_i=\sum_{i=1}^n  \sum_{j \in C}p_ij\]</span></p>
</blockquote>
<p>最后result的衡量方式使用false positive rate(FPR)。</p>
<h3 id="transformer">Transformer</h3>
<ul>
<li>Vaswani A, Shazeer N, Parmar N, et al. Attention is all you
need[C]//Advances in neural information processing systems. 2017:
5998-6008.</li>
<li>Google</li>
</ul>
<p>RNN，LSTM，GNU是处理序列模型的几种最优方法。然而循环神经网络中总是沿着词元位置进行计算，这种<strong>顺序性阻碍了训练的并行化</strong>，这也严重影响了内存对batch的限制程度。因此<strong>Transformer</strong>完全依赖<strong>Self-Attention</strong>来抽取输入和输出的全局关系。并且能有更好的并行化。</p>
<h3 id="external-attention">External Attention</h3>
<ul>
<li>Guo M H, Liu Z N, Mu T J, et al. Beyond self-attention: External
attention using two linear layers for visual tasks[J]. arXiv preprint
arXiv:2105.02358, 2021.</li>
<li>清华</li>
</ul>
<p>自注意力机制在同一个样本内,
任意一个部位的特征都可以聚合所有位置的特征进行加权输出。但是自注意力拥有<strong>二次复杂度</strong>,
并且<strong>不能计算多个样本之间的潜在联系</strong>。</p>
<p>External-Attention(EAT) 希望在学习某个数据集时,
能够找到多个样本之间的潜在联系。其通过保持一定的<strong>key
memory</strong>,
以找到跨越所有样本的最具有辨识性的特征。这种思想类似于sparse coding 和
dictionary learning。并且由于key memory设计的很小,
因此EAT计算上具有O(n)的复杂度,
比起自注意力<strong>高效</strong>很多。</p>
<h3 id="vision-transformer-vit">Vision Transformer (ViT)</h3>
<ul>
<li>Dosovitskiy A, Beyer L, Kolesnikov A, et al. An Image is Worth 16x16
Words: Transformers for Image Recognition at Scale[C]//International
Conference on Learning Representations. 2020.</li>
<li>Google</li>
</ul>
<figure>
<img src="https://i.imgur.com/CbvUBIo.png" alt="模型结构" />
<figcaption aria-hidden="true">模型结构</figcaption>
</figure>
<p>第一篇CV上的Transformer：</p>
<ul>
<li><strong>patch embedding</strong>:
为了仿照NLP的输入结构，将图像划分为多个patches，并且展平为一维序列。由于Transformer层的输入长度固定为D，因此原patches组成的特征序列长度为N，通过线性层将长度映射成D。</li>
<li><strong>position embeddings</strong>：由于2D的position
embeddings没有体现出优越性，因此还是使用了标准的1D可学习的position
embeddings。</li>
<li><strong>Hybrid Architecture</strong>:
patches的特征可以通过CNN来进行提取。</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
\mathbf{z}_{0} &amp;=\left[\mathbf{x}_{\text {class }} ;
\mathbf{x}_{p}^{1} \mathbf{E} ; \mathbf{x}_{p}^{2} \mathbf{E} ; \cdots ;
\mathbf{x}_{p}^{N} \mathbf{E}\right]+\mathbf{E}_{p o s}, &amp; &amp;
\mathbf{E} \in \mathbb{R}^{\left(P^{2} \cdot C\right) \times D},
\mathbf{E}_{p o s} \in \mathbb{R}^{(N+1) \times D} \\
\mathbf{z}_{\ell}^{\prime}
&amp;=\operatorname{MSA}\left(\operatorname{LN}\left(\mathbf{z}_{\ell-1}\right)\right)+\mathbf{z}_{\ell-1},
&amp; &amp; \ell=1 \ldots L \\
\mathbf{z}_{\ell}
&amp;=\operatorname{MLP}\left(\operatorname{LN}\left(\mathbf{z}_{\ell}^{\prime}\right)\right)+\mathbf{z}_{\ell}^{\prime},
&amp; &amp; \ell=1 \ldots L \\
\mathbf{y} &amp;=\operatorname{LN}\left(\mathbf{z}_{L}^{0}\right) &amp;
&amp;
\end{aligned}
\]</span></p>
<blockquote>
<p>分辨率单一，计算效率低。 class token是什么？</p>
</blockquote>
<h3 id="swim-transformer">Swim Transformer</h3>
<ul>
<li>Liu Z, Lin Y, Cao Y, et al. Swin transformer: Hierarchical vision
transformer using shifted windows[C]//Proceedings of the IEEE/CVF
International Conference on Computer Vision. 2021: 10012-10022.</li>
</ul>
<p>在ViT中，自注意力是全局计算的，但是图片分辨率比起句子往往较大，因此带来了计算效率低的问题。locality一直是视觉里的重要建模方式，因此这篇文章将图片切分为<strong>不重合的local
window</strong>，并且在local
window内部进行注意力计算。为了让window之间有信息交换，在相邻两层使用<strong>不同的window划分</strong>。</p>
<p>金字塔层次化Transformer。</p>
<h3 id="layer-normalization">Layer Normalization</h3>
<ul>
<li>Ba J L, Kiros J R, Hinton G E. Layer normalization[J]. arXiv
preprint arXiv:1607.06450, 2016.</li>
</ul>
<h3 id="pre-layer-normalization">Pre Layer Normalization</h3>
<ul>
<li>Xiong R, Yang Y, He D, et al. On layer normalization in the
transformer architecture[C]//International Conference on Machine
Learning. PMLR, 2020: 10524-10533.</li>
</ul>
<p>本文分析了Transformer模型的初始化warmup问题，将layer
normalization从原本的残差后移到了残差前，从实验和理论证明了pre-LN不需要warmup，并且收敛更好。</p>
<h3 id="localvit">LocalViT</h3>
<ul>
<li>Li Y, Zhang K, Cao J, et al. Localvit: Bringing locality to vision
transformers[J]. arXiv preprint arXiv:2104.05707, 2021.</li>
<li>苏黎世联邦理工</li>
</ul>
<p>在Transformer的FeedForward层中添加inverted residual block以及
depth-wise 卷积来增加local能力。</p>
<p>FeedForward层可以有助于增加Transformer结构的泛化能力。</p>
<figure>
<img src="https://i.imgur.com/msRVrEW.png" alt="local feedforward" />
<figcaption aria-hidden="true">local feedforward</figcaption>
</figure>
<h3 id="mobilenet">MobileNet</h3>
<ul>
<li>Howard A G, Zhu M, Chen B, et al. Mobilenets: Efficient
convolutional neural networks for mobile vision applications[J]. arXiv
preprint arXiv:1704.04861, 2017.</li>
<li>Google</li>
</ul>
<p><strong>Depth wise
卷积</strong>：将原来的<strong>多输入通道+多输出通道</strong>卷积拆分为
<strong>逐通道卷积+1D卷积通道变换</strong>，极大节省计算量。其计算量从<span
class="math inline">\(D_K \times D_K \times M \times N \times D_F \times
D_F\)</span>变为<span class="math inline">\(D_K \times D_K \times M
\times D_F \times D_F+M \times N \times D_F \times
D_F\)</span>，论文模型在ImageNet上参数量大约缩小为1/10，准确率下降1%。</p>
<figure>
<img src="https://i.imgur.com/2tkPCO8.png" alt="DepthWise Conv" />
<figcaption aria-hidden="true">DepthWise Conv</figcaption>
</figure>
<h3 id="resnet">ResNet</h3>
<ul>
<li>He K, Zhang X, Ren S, et al. Deep residual learning for image
recognition[C]//Proceedings of the IEEE conference on computer vision
and pattern recognition. 2016: 770-778.</li>
</ul>
<h3 id="位置编码">位置编码</h3>
<ul>
<li>T5编码 Raffel C, Shazeer N, Roberts A, et al. Exploring the limits
of transfer learning with a unified text-to-text transformer[J]. arXiv
preprint arXiv:1910.10683, 2019.</li>
<li>Ke G, He D, Liu T Y. Rethinking Positional Encoding in Language
Pre-training[C]//International Conference on Learning Representations.
2020.</li>
<li>相对位置编码基础 Shaw P, Uszkoreit J, Vaswani A. Self-Attention with
Relative Position Representations[C]//Proceedings of the 2018 Conference
of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 2 (Short Papers). 2018:
464-468.</li>
<li>训练式编码 Gehring J, Auli M, Grangier D, et al. Convolutional
sequence to sequence learning[C]//International Conference on Machine
Learning. PMLR, 2017: 1243-1252.</li>
</ul>
<h3 id="训练优化">训练优化</h3>
<ul>
<li><p>Goyal P, Dollár P, Girshick R, et al. Accurate, large minibatch
sgd: Training imagenet in 1 hour[J]. arXiv preprint arXiv:1706.02677,
2017.</p></li>
<li><p>Smith L N. Cyclical learning rates for training neural
networks[C]//2017 IEEE winter conference on applications of computer
vision (WACV). IEEE, 2017: 464-472.</p></li>
</ul>
<p>传统的方法中认为学习率应该是一个单调缓慢减少的数，然而这篇文章指出在一个<strong>周期范围变化的学习率</strong>可以有更好的效果。其循环变化方式可以有三角式、余弦式，线性式等。</p>
<p><strong>原理</strong>：通常造成loss下降困难的地方是
<strong>鞍点</strong> 而不是简单的
<strong>极小点</strong>，在鞍点附近梯度极小，导致更新缓慢。此时就可以通过增大学习率来跳出鞍点。</p>
<p><strong>确定循环周期</strong>：实验表明半周期设置为2~10个epoch*iterations比较好。(按iteration迭代，而不是按epoch迭代)。最好使用3个以上的epoch来代替一个常量学习率下的epoch。</p>
<p><strong>确定学习率上下界</strong>：所谓LR
test，运行几个单独的epoch，同时线性增大学习率，观察准确率的变化，找到第一个增加点，和波动之前的最后一个点。</p>
<ul>
<li>Loshchilov I, Hutter F. Sgdr: Stochastic gradient descent with warm
restarts[J]. arXiv preprint arXiv:1608.03983, 2016.</li>
</ul>
<p>实验表明，使用warm
restart的学习率更新方法，收敛速度比寻常方法可以快2~4倍。其提出了<strong>余弦退火学习率调度器</strong>：</p>
<p><span class="math display">\[
\eta_{t}=\eta_{\min }^{i}+\frac{1}{2}\left(\eta_{\max }^{i}-\eta_{\min
}^{i}\right)\left(1+\cos \left(\frac{T_{\text {cur }}}{T_{i}}
\pi\right)\right),
\]</span></p>
<p>论文使用参数<span class="math inline">\(T_o=10\)</span>,<span
class="math inline">\(T_{mult}=2\)</span>。</p>
<h3 id="can-attention-enable-mlps-to-catch-up-with-cnns">Can attention
enable MLPs to catch up with CNNs</h3>
<ul>
<li>Guo M H, Liu Z N, Mu T J, et al. Can attention enable MLPs to catch
up with CNNs?[J]. Computational Visual Media, 2021, 7(3): 283-288.</li>
</ul>
<p>比较了几个新兴的MLP，CNN，Transformer模型，总结了以下几个共同点：</p>
<ul>
<li>通过将图片划分为Patch，可以更好地捕捉局部结构。</li>
<li>注意力中的辅助结构也可以考虑应用在非注意力模型上，比如Multi-Head</li>
<li>Residual 结构对于所有模型都很重要</li>
<li>局部计算的CNN会导致归纳偏差(Inductive
Bias)，而一维卷积和全域计算结构可以减少归纳偏差。</li>
</ul>
<h3 id="action-recognition-based-on-a-bag-of-3d-points">Action
recognition Based on A Bag of 3D Points</h3>
<ul>
<li>Li W, Zhang Z, Liu Z. Action recognition based on a bag of 3d
points[C]//2010 IEEE computer society conference on computer vision and
pattern recognition-workshops. IEEE, 2010: 9-14.</li>
<li>CVPRW</li>
<li>University of Wollongong</li>
</ul>
<p>第一，其提出可以用整个点云bag来表示动作序列。并且点出每一帧之间，点的个数可能不同，并且点之间没有对应关系。更进一步，其假设一个点云帧是一个
<strong>混合高斯模型</strong>，可以通过一系列的混合高斯分布来描述：</p>
<p><span class="math display">\[
p(x \mid \omega)=\prod_{i=1}^{m} \sum_{t=1}^{Q} \pi_{t}^{\omega}
g\left(q_{i}, \mu_{t}^{\omega}, \Sigma_{t}^{\omega}\right)
\]</span></p>
<p>另外，由于点的数量太多容易造成noise和计算问题，需要对点进行采样。在2D的经验里，人体的边缘轮廓是最重要的形状信息。进而在3D中，提出了一种点云的<strong>投影采样法</strong>：通过将点云投影到三个正交2D面，再在2D上进行轮廓采样。</p>
<figure>
<img src="https://i.imgur.com/Fi3BjWH.png" alt="投影采样" />
<figcaption aria-hidden="true">投影采样</figcaption>
</figure>
<h3
id="dpdist-comparing-point-clouds-using-deep-point-cloud-distance">DPDist:
Comparing Point Clouds Using Deep Point Cloud Distance</h3>
<ul>
<li>Urbach D, Ben-Shabat Y, Lindenbaum M. DPDist: Comparing point clouds
using deep point cloud distance[C]//European Conference on Computer
Vision. Springer, Cham, 2020: 545-560.</li>
<li>澳大利亚国立大学</li>
</ul>
<p>本文提出了一种衡量点云距离的深度学习方法，不同于传统方法，其衡量的是点云A到点云B的surface
representation的距离。介绍了几种传统点云距离度量方法，以及新的基于深度学习的改进方法。</p>
<p><strong>Hausdorff distance</strong> 距离：点到点距离<span
class="math inline">\(d(x,y)\)</span>，点到点云距离<span
class="math inline">\(D(x,S)\)</span>，点云到点云距离<span
class="math inline">\(D_H(S_A,S_B)\)</span>。</p>
<p><span class="math display">\[
\begin{aligned}
d(x, y) &amp;=\|x-y\|_{2} \\
D(x, S) &amp;=\min _{y \in S} d(x, y) \\
\mathcal{D}_{H}\left(S_{A}, S_{B}\right)&amp;=\max \left\{\max _{a \in
S_{A}} D\left(a, S_{B}\right), \max _{b \in S_{B}} D\left(b,
S_{A}\right)\right\}
\end{aligned}
\]</span></p>
<p><strong>Chamfer
distance</strong>：相比于Hausdorff距离，Chamfer取平均距离而不是取最大距离。</p>
<p><span class="math display">\[
\mathcal{D}_{C D}\left(S_{A}, S_{B}\right)=\frac{1}{N_{A}} \sum_{a \in
S_{A}} \min _{y \in S_{B}} d(a, y)^{2}+\frac{1}{N_{B}} \sum_{b \in
S_{B}} \min _{y \in S_{A}} d(b, y)^{2}
\]</span></p>
<p>Earth Mover's Distance :
在点云B中确定一种离点云A最近的映射关系，再累加欧拉距离。</p>
<p><span class="math display">\[
\mathcal{D}_{E M D}\left(S_{A}, S_{B}\right)=\min _{\xi: S_{A}
\rightarrow S_{B}} \sum_{a \in S_{A}}\|a-\xi(a)\|_{2}
\]</span></p>
<h3 id="siamese-neural-network-triplet-network">Siamese Neural Network
&amp; Triplet Network</h3>
<ul>
<li>Bromley J, Guyon I, LeCun Y, et al. Signature verification using a"
siamese" time delay neural network[J]. Advances in neural information
processing systems, 1993, 6.</li>
</ul>
<p>孪生神经网络，用于衡量两个小样本输入的相似程度。在输入数据属于训练数据集的情况下，通常我们可以通过数据识别分类来比较两个数据。然而对于小样本数据，其数据量根本不够成为训练数据集，甚至这个数据可能只有一份，此时则无法通过训练识别来进行分类。</p>
<p>为了能够度量训练集以外的输入数据相似性，孪生神经网络在训练<strong>特征提取</strong>网络的基础上，还增加了<strong>同类相近、异类排斥</strong>的结构，以便于强调网络的<strong>数据匹配</strong>能力。其将一对输入<span
class="math inline">\(X_1,X_2\)</span>给到同一个神经网络提取特征向量<span
class="math inline">\(F_1,F_2\)</span>，并计算两个特征向量的相似度。如果是同类别的输入对，则期望相似度输出为1，如果是不同类别的输入对则期望输出0。</p>
<figure>
<img src="https://i.imgur.com/eqmq6lr.png" alt="孪生神经网络模型图" />
<figcaption aria-hidden="true">孪生神经网络模型图</figcaption>
</figure>
<p>损失函数Contrastive Loss：<span
class="math inline">\(D_W\)</span>为输入对的特征向量的距离度量，<span
class="math inline">\(Y\)</span>指明输入对是同类还是异类。即通过<span
class="math inline">\(Y\)</span>可以选择启用损失函数里的<strong>同类损失部分</strong>或者<strong>异类损失部分</strong>。<strong>同类距离需要尽可能的小，异类距离需要尽可能的大</strong>。</p>
<p><span class="math display">\[
D_{w}(X_{1}, X_{2}) = ||G(X_{1}) - G(X_{2})|| \\
\mathcal{L} = (1-Y)\frac{1}{2}(D_W)^2+(Y)\frac{1}{2}\{max(0, m-D_W)\}^2
\]</span></p>
<p>在孪生网络中每一对输入要么是同类的，要么是异类的，即同类和异类的训练需要分PASS进行。而在Triplet三生网络中则将输入进行扩展，一组输入包含三个数据，既有同类，又有异类，进而可以将训练过程放在一个PASS完成。</p>
<figure>
<img src="https://i.imgur.com/QWhZZWr.png" alt="三生神经网络模型图" />
<figcaption aria-hidden="true">三生神经网络模型图</figcaption>
</figure>
<p>同样，其损失函数<strong>三元组损失</strong>也改造成了一个PASS的形式：其本质上衡量了<span
class="math inline">\((d_+,d_-)\)</span>与<span
class="math inline">\((0,1)\)</span>的向量距离。</p>
<p><span class="math display">\[
d_{+} = \frac{e^{\Vert Net(x)-Net(x^{+})\Vert _2}}{e^{\Vert
Net(x)-Net(x^{+})\Vert _2}+e^{\Vert Net(x)-Net(x^{-})\Vert _2}} \\
Loss(d_{+},d_{-}) = \Vert (d_{+} , d_{-}-1)\Vert _2^2 = const\cdot d_+^2
\\
\]</span></p>
<h3
id="action-density-based-frame-sampling-for-human-action-recognition-in-videos">Action
density based frame sampling for human action recognition in videos</h3>
<ul>
<li>Lin J, Mu Z, Zhao T, et al. Action density based frame sampling for
human action recognition in videos[J]. Journal of Visual Communication
and Image Representation, 2023, 90: 103740.</li>
<li>西安交大 JVCIR 一区</li>
</ul>
<figure>
<img src="https://i.imgur.com/0Cjr6hr.png" alt="框架图" />
<figcaption aria-hidden="true">框架图</figcaption>
</figure>
<p>视频帧采样优化的论文。主要想法在于把视频分为
信息密度大的clip和信息密度小的clip，并且分别处理采样。整个流程分为三个环节：帧打分、区间划分、区间采样。</p>
<p>Action density
determination：即给原始视频的每一帧进行密度分数评估：</p>
<p><span class="math display">\[
\left\{\begin{array}{l}
D_n=\sqrt{\sum\left|E(x, y)+I_{\text {fout }}(x, y)\right|^2} \\
E(x, y)=\left\{\begin{array}{cl}
\alpha \cdot I_{\text {fout }}(x, y), &amp; \left(I_{\text {bout }}(x,
y)=1\right) \\
0, &amp; (Else)
\end{array}\right.
\end{array}\right.
\]</span></p>
<p>其中<span class="math inline">\(I_{\text {fout }}(x,
y)\)</span>为<span
class="math inline">\((x,y)\)</span>的像素在相邻帧之中的距离大小。<span
class="math inline">\(I_{\text {bout }}(x,
y)\)</span>为图像前景和背景的距离大小。</p>
<p>Focused-clips division mechanism：
有了每一帧的打分之后，即可以划定一个分数阈值，来将原始视频区分为高密度clip和低密度clip。本文选择了分数Top
K的平均分数作为阈值，然后评估所有的连续帧。高于阈值的连续帧作为高密度clip，低于阈值的连续帧则作为低密度clip
(注意高密度低密度都不止有一组，整个视频被阈值切割成了多个clip片段)，
这样采样时可以选择高密度的clip采样频率更高，低密度的采样频率更低。</p>
<p>Reinforcement learning based frame sampling (RLFS) mechanism：
强化学习采样法。本质即惩罚采样连续的帧，鼓励间断性采样。</p>
<h3
id="scsampler-sampling-salient-clips-from-video-for-efficient-action-recognition">SCSampler:
Sampling Salient Clips from Video for Efficient Action Recognition</h3>
<ul>
<li>Korbar B, Tran D, Torresani L. Scsampler: Sampling salient clips
from video for efficient action recognition[C]//Proceedings of the
IEEE/CVF International Conference on Computer Vision. 2019:
6232-6242.</li>
<li>ICCV FacebookAI</li>
</ul>
<p>核心是对于视频提供一个帧评分采样器
SCsampler。论文希望通过优化采样后，可以节省长视频中大量无关内容的计算，并且提升准确率。</p>
<p>SCsampler的实现在于如何给每一帧进行saliency
score的评分计算，论文给出了两种实现思路： - 朴素思路是计算classification
score，即计算每一帧对输出label的帮助程度/相关性，论文通过预训练一个针对每一帧的分类网络<span
class="math inline">\(h\)</span>来实现，saliency score即<span
class="math inline">\(h\)</span>对所有label最大的一个预测概率。 -
论文优化后的方法是希望通过 打分映射<span
class="math inline">\(s\)</span> 和 定制loss 来训练可学习的打分环节。
<span class="math display">\[
\ell\left(\phi_n^{(i)}, \phi_n^{(j)}\right)=\max \left(-z_n^{(i,
j)}\left[s\left(\phi_n^{(i)}\right)-s\left(\phi_n^{(j)}\right)+\eta\right],
0\right)
\]</span> 这个loss使用从原视频中采样出的clips对<span
class="math inline">\(v_n^i,v_n^j\)</span>来比较计算，其中如果<span
class="math inline">\(v_n^i\)</span>对label预测的概率比<span
class="math inline">\(v_n^j\)</span>大，那么<span
class="math inline">\(z_n^{(i, j)}\)</span>为 1 ，否则是-1。
这个loss本质上在鼓励saliency score的打分映射<span
class="math inline">\(s\)</span>对两个clip的评分差距接近classification
score。</p>
<p>实验效果来看，对于视频，第一种朴素思路比第二种效果好很多。对于音频，第二种稍微好一点点。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%BA%8F%E5%88%97%E5%AD%A6%E4%B9%A0/" rel="tag"># 序列学习</a>
              <a href="/tags/%E7%82%B9%E4%BA%91/" rel="tag"># 点云</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/01/08/SkeletonTransformer/" rel="prev" title="SkeletonTransformer">
      <i class="fa fa-chevron-left"></i> SkeletonTransformer
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/02/17/Point4DTransformer/" rel="next" title="Point4DTransformer">
      Point4DTransformer <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%82%B9%E4%BA%91%E7%A9%BA%E9%97%B4%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.</span> <span class="nav-text">点云空间学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#pointnet-deep-learning-on-point-sets-for-3d-classification-and-segmentation"><span class="nav-number">1.1.</span> <span class="nav-text">PointNet:
Deep Learning on Point Sets for 3D Classification and Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space"><span class="nav-number">1.2.</span> <span class="nav-text">PointNet++:
Deep Hierarchical Feature Learning on Point Sets in a Metric Space</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pointconv"><span class="nav-number">1.3.</span> <span class="nav-text">PointConv</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pointcnn"><span class="nav-number">1.4.</span> <span class="nav-text">PointCNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kpconv"><span class="nav-number">1.5.</span> <span class="nav-text">KPConv</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pointnet-based-hand-gesture-recognition"><span class="nav-number">1.6.</span> <span class="nav-text">PointNet-Based-Hand-Gesture-Recognition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pointweb"><span class="nav-number">1.7.</span> <span class="nav-text">PointWeb</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#deep-hough-voting-for-3d-object-detection-in-point-clouds"><span class="nav-number">1.8.</span> <span class="nav-text">Deep
Hough Voting for 3D Object Detection in Point Clouds</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pct-point-cloud-transformer"><span class="nav-number">1.9.</span> <span class="nav-text">PCT: Point cloud transformer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#point-transformer"><span class="nav-number">1.10.</span> <span class="nav-text">Point Transformer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rs-conv"><span class="nav-number">1.11.</span> <span class="nav-text">RS-Conv</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pointcmt"><span class="nav-number">1.12.</span> <span class="nav-text">PointCMT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dual-transformer"><span class="nav-number">1.13.</span> <span class="nav-text">Dual Transformer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pointswd"><span class="nav-number">1.14.</span> <span class="nav-text">PointSWD</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%82%B9%E4%BA%91%E6%97%B6%E9%97%B4%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.</span> <span class="nav-text">点云时间学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#flownet3d"><span class="nav-number">2.1.</span> <span class="nav-text">FlowNet3D</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#meteornet"><span class="nav-number">2.2.</span> <span class="nav-text">MeteorNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#minkowskinet"><span class="nav-number">2.3.</span> <span class="nav-text">MinkowskiNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#caspr"><span class="nav-number">2.4.</span> <span class="nav-text">CaSPR</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pointlstm-an-efficient-pointlstm-for-point-clouds-based-gesture-recognition"><span class="nav-number">2.5.</span> <span class="nav-text">PointLSTM:
An Efficient PointLSTM for Point Clouds Based Gesture Recognition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pstnet"><span class="nav-number">2.6.</span> <span class="nav-text">PSTNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sequentialpointnet-a-strong-parallelized-point-cloud-sequence-network-for-3d-action-recognition"><span class="nav-number">2.7.</span> <span class="nav-text">SequentialPointNet:
A strong parallelized point cloud sequence network for 3D action
recognition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#point4dtransformer"><span class="nav-number">2.8.</span> <span class="nav-text">Point4DTransformer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pst-transformer"><span class="nav-number">2.9.</span> <span class="nav-text">PST-Transformer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#geometrymotion-net"><span class="nav-number">2.10.</span> <span class="nav-text">GeometryMotion-Net</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tranquil-clouds"><span class="nav-number">2.11.</span> <span class="nav-text">Tranquil clouds</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pointpwc-net"><span class="nav-number">2.12.</span> <span class="nav-text">PointPWC-Net</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spcm-net"><span class="nav-number">2.13.</span> <span class="nav-text">SPCM-Net</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pstt"><span class="nav-number">2.14.</span> <span class="nav-text">PSTT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hyperpointnet"><span class="nav-number">2.15.</span> <span class="nav-text">HyperPointnet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#virtualactionnet"><span class="nav-number">2.16.</span> <span class="nav-text">VirtualActionNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#action-recognition-from-silhouette-sequences"><span class="nav-number">2.17.</span> <span class="nav-text">Action recognition
from silhouette sequences</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#facial-action-analysis"><span class="nav-number">2.18.</span> <span class="nav-text">Facial action analysis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#asta3dcnns"><span class="nav-number">2.19.</span> <span class="nav-text">ASTA3DCNNs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#aspnet"><span class="nav-number">2.20.</span> <span class="nav-text">ASPNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pointmotionnet"><span class="nav-number">2.21.</span> <span class="nav-text">PointMotionNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#geometrymotion-transformer"><span class="nav-number">2.22.</span> <span class="nav-text">GeometryMotion-Transformer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#continuous-body-and-hand-gesture-recognition-for-natural-human-computer-interaction"><span class="nav-number">2.23.</span> <span class="nav-text">Continuous
Body and Hand Gesture Recognition for Natural Human-Computer
Interaction</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%85%B6%E4%BB%96%E4%B8%89%E7%BB%B4%E6%95%B0%E6%8D%AE"><span class="nav-number">3.</span> <span class="nav-text">基于其他三维数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#dgcnn"><span class="nav-number">3.1.</span> <span class="nav-text">DGCNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ddgcn-a-dynamic-directed-graph-convolutional-network-for-action-recognition"><span class="nav-number">3.2.</span> <span class="nav-text">DDGCN:
A Dynamic Directed Graph Convolutional Network for Action
Recognition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#skeletontransformer"><span class="nav-number">3.3.</span> <span class="nav-text">SkeletonTransformer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dv"><span class="nav-number">3.4.</span> <span class="nav-text">3DV</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#histogram-of-motion-trajectory-feature"><span class="nav-number">3.5.</span> <span class="nav-text">Histogram of motion
trajectory feature</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#d-rans"><span class="nav-number">3.6.</span> <span class="nav-text">3D RANs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fine-grained"><span class="nav-number">3.7.</span> <span class="nav-text">Fine-grained</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD"><span class="nav-number">4.</span> <span class="nav-text">基础设施</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%82%B9%E4%BA%91%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B0"><span class="nav-number">4.1.</span> <span class="nav-text">点云分类综述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#deep-metric-learning-a-survey"><span class="nav-number">4.2.</span> <span class="nav-text">deep metric learning a
survey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#human-motion-analysis-metric"><span class="nav-number">4.3.</span> <span class="nav-text">Human motion analysis metric</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transformer"><span class="nav-number">4.4.</span> <span class="nav-text">Transformer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#external-attention"><span class="nav-number">4.5.</span> <span class="nav-text">External Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#vision-transformer-vit"><span class="nav-number">4.6.</span> <span class="nav-text">Vision Transformer (ViT)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#swim-transformer"><span class="nav-number">4.7.</span> <span class="nav-text">Swim Transformer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#layer-normalization"><span class="nav-number">4.8.</span> <span class="nav-text">Layer Normalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pre-layer-normalization"><span class="nav-number">4.9.</span> <span class="nav-text">Pre Layer Normalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#localvit"><span class="nav-number">4.10.</span> <span class="nav-text">LocalViT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mobilenet"><span class="nav-number">4.11.</span> <span class="nav-text">MobileNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#resnet"><span class="nav-number">4.12.</span> <span class="nav-text">ResNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="nav-number">4.13.</span> <span class="nav-text">位置编码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E4%BC%98%E5%8C%96"><span class="nav-number">4.14.</span> <span class="nav-text">训练优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#can-attention-enable-mlps-to-catch-up-with-cnns"><span class="nav-number">4.15.</span> <span class="nav-text">Can attention
enable MLPs to catch up with CNNs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#action-recognition-based-on-a-bag-of-3d-points"><span class="nav-number">4.16.</span> <span class="nav-text">Action
recognition Based on A Bag of 3D Points</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dpdist-comparing-point-clouds-using-deep-point-cloud-distance"><span class="nav-number">4.17.</span> <span class="nav-text">DPDist:
Comparing Point Clouds Using Deep Point Cloud Distance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#siamese-neural-network-triplet-network"><span class="nav-number">4.18.</span> <span class="nav-text">Siamese Neural Network
&amp; Triplet Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#action-density-based-frame-sampling-for-human-action-recognition-in-videos"><span class="nav-number">4.19.</span> <span class="nav-text">Action
density based frame sampling for human action recognition in videos</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#scsampler-sampling-salient-clips-from-video-for-efficient-action-recognition"><span class="nav-number">4.20.</span> <span class="nav-text">SCSampler:
Sampling Salient Clips from Video for Efficient Action Recognition</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Sitch"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Sitch</p>
  <div class="site-description" itemprop="description">做好自己的现在</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">124</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">27</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/alobal" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;alobal" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:778377698@qq.com" title="E-Mail → mailto:778377698@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://steamcommunity.com/id/sitchzou/" title="Steam → https:&#x2F;&#x2F;steamcommunity.com&#x2F;id&#x2F;sitchzou&#x2F;" rel="noopener" target="_blank"><i class="fab fa-steam fa-fw"></i>Steam</a>
      </span>
  </div>



      </div>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Sitch</span>
</div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
// var pjax = new Pjax({
//   selectors: [
//     'head title',
//     '#page-configurations',
//     '.content-wrap',
//     '.post-toc-wrap',
//     '.languages',
//     '#pjax'
//   ],
//   switches: {
//     '.post-toc-wrap': Pjax.switches.innerHTML
//   },
//   analytics: false,
//   cacheBust: false,
//   scrollTo : !CONFIG.bookmark.enable
// });

// window.addEventListener('pjax:success', () => {
//   document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
//     var code = element.text || element.textContent || element.innerHTML || '';
//     var parent = element.parentNode;
//     parent.removeChild(element);
//     var script = document.createElement('script');
//     if (element.id) {
//       script.id = element.id;
//     }
//     if (element.className) {
//       script.className = element.className;
//     }
//     if (element.type) {
//       script.type = element.type;
//     }
//     if (element.src) {
//       script.src = element.src;
//       // Force synchronous loading of peripheral JS.
//       script.async = false;
//     }
//     if (element.dataset.pjax !== undefined) {
//       script.dataset.pjax = '';
//     }
//     if (code !== '') {
//       script.appendChild(document.createTextNode(code));
//     }
//     parent.appendChild(script);
//   });
//   NexT.boot.refresh();
//   // Define Motion Sequence & Bootstrap Motion.
//   if (CONFIG.motion.enable) {
//     NexT.motion.integrator
//       .init()
//       .add(NexT.motion.middleWares.subMenu)
//       .add(NexT.motion.middleWares.postList)
//       .bootstrap();
//   }
//   NexT.utils.updateSidebarPosition();
// });
</script>




  
  <script data-pjax>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="/js/local-search.js"></script>













    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'PcR31yj1TEn6z0UqyTXNP6NN-gzGzoHsz',
      appKey     : 'LEQbv9t6GJ3BAdMOMXDh0blJ',
      placeholder: "【留言板】  欢迎用你的脸滚一滚键盘~\n支持markdown语法,没有邮件提醒可能回复稍慢，抱歉~\n",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : 'https://pcr31yj1.lc-cn-n1-shared.com'
    });
  }, window.Valine);
});
</script>

    </div>


</html>

<!--崩溃欺骗-->


